\documentclass[12pt]{article}

\RequirePackage{amsmath}
\RequirePackage{amsthm}
\RequirePackage{amssymb}
\RequirePackage[mathscr]{eucal}
\RequirePackage{mathtools}
\RequirePackage{etoolbox}

\usepackage[red]{zhoucx-notation}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\renewcommand{\qedsymbol}{\hfill\rule{2mm}{2mm}}

\pagestyle{fancy}
\fancyhf{}
\setlength{\headheight}{15pt}
\rhead{\textsf{Chapter 27, Self-Organizing Maps}}
\lhead{\textsf{Chenxi Zhou}}
\renewcommand{\headrulewidth}{1pt}
\cfoot{\thepage}

\newcommand{\titlebox}[4]{
\begin{tcolorbox}[colback = blue!5!white!95, colframe = blue!70!black
% colback = yellow!30!white, colframe = yellow!70!black 
]
  \noindent \textbf{ #1 } \hfill \textit{#2} 
  \begin{center}
  	 \LARGE{\textbf{#3}}
  \end{center}
\textbf{Chapter:} \textit{#4} \hfill \textbf{Prepared by:} \textit{Chenxi Zhou}
\end{tcolorbox}
}

\begin{document}

\titlebox{Notes on Statistical and Machine Learning}{}{Self-Organizing Maps}{27}
\thispagestyle{plain}

\vspace{10pt}

This note is prepared based on 
\begin{itemize}
	\item \textit{Chapter 14, Unsupervised Learning} in \textcite{Friedman2001-np}, and 
	\item \textit{Chapter 12, Clustering Analysis} in \textcite{Izenman2009-jk}. 
\end{itemize}


\section*{I. Introduction}

\begin{enumerate}[label=\textbf{\arabic*.}]

	\item \textbf{Overview:} \emph{Self-organizing map} (SOM) can be viewed as a constrained version of $K$-means clustering in which the prototypes are encouraged to lie in a one- or two-dimensional manifold in the feature. 

	\item \textbf{Primary Use:} The primary use of an SOM is 
	\begin{enumerate}
		\item in reducing high-dimensional data to a lower-dimensional nonlinear manifold, and 
		\item in displaying graphically the results of such data reduction. 
	\end{enumerate}
	
	\item \textbf{Goal:} The goal of SOM is to map the projected data to discrete interconnected nodes, where each node represents a grouping or cluster of relatively homogeneous points. 
	
	\item \textbf{End Product:} The end product of the SOM algorithm is a image called an \emph{SOM plot}. The SOM plot is displayed in output space and consists of a grid (or network) of a large number of interconnected nodes. 
	
	\textit{Remark.} In two dimensions, the nodes are typically arranged as a square, rectangular, or hexagonal grid. 
	
	\item \textbf{Example of an SOM Plot in Two-dimensional Case:} Consider to create an SOM plot in two-dimensional setting with rectangular grids. Let the set of rows be $\calK_1 := \sets{1, 2, \cdots, K_1}$ and the set of columns be $\calK_2 := \sets{1, 2, \cdots, K_2}$, where $K_1$ (the height) and $K_2$ (the width) are chosen by the user. 
	
	Then, a node within the SOM plot is defined by its coordinates, $\parens{k_1, k_2} \in \calK_1 \times \calK_2$. It will be convenient to map the collection of nodes into an ordered sequence, so that the node $\parens{k_1, k_2}$ is relabeled as $k := \parens{k_1 - 1} K_2 + k_2 \in \calK$, where $\calK := \sets{1, 2, \cdots, K_1 K_2}$. 
	
	\textit{Remark.} The total number of nodes, $K := K_1 K_2$, is usually chosen by trial and error, initially much larger than the suspected number of clusters in the data. 
	
	After an initial SOM analysis, one can reconfigure the SOM by reducing the number of row and column nodes so as to reduce the value of $K$. 

\end{enumerate}


\section*{II. On-line SOM Algorithm}

\begin{enumerate}[label=\textbf{\arabic*.}]
	
	\item \textbf{General Idea of SOM Algorithm:} We associate with the $k$-th node in an SOM plot a representative in input space, $\bm_k \in \Real^p$, where $k \in \calK$ and $p$ is the dimensionality of the feature variables. 
	
	\item \textbf{Initialization of $\sets{\bm_k}_{k=1}^K$:} It is usual to initialize the components of $\bm_k$ to be random numbers, for all $k \in \calK$. 
	
	\item \textbf{$c$-Grid Neighbor and $c$-Neighborhood Set:} Let $c > 0$ be fixed. A node $k' \in \calK$ is defined to be a \emph{$c$-grid neighbor} of the node $k \in \calK$ if the distance between $\bm_k$ and $\bm_{k'}$ is smaller than a given threshold $c$. The set of nodes that are $c$-grid neighbors of the node $k$, denoted by $\calN_c \parens{k}$, is called the \emph{$c$-neighborhood set} for that node. 
	
	\item \textbf{On-line Version of SOM Algorithm:} The on-line version of SOM algorithm processes each observation $\bx_i \in \Real^p$ individually and sequentially. The complete algorithm is shown in Algorithm \ref{algo-online-som}. 
	
	\begin{minipage}{\linewidth}
		\begin{algorithm}[H]
			\caption{On-line Version of SOM Algorithm}\label{algo-online-som}
			\begin{algorithmic}[1]
				\REQUIRE map size, that is, $K_1$ and $K_2$; 
				\REQUIRE initializing points of all representatives $\sets{\bm_k}_{k=1}^K$; 
				\REQUIRE data $\bx_1, \bx_2, \cdots, \bx_n$. 
				
				\vspace{10pt}
				\FOR{a sequence of data points $\bx_1, \bx_2, \bx_3, \cdots$, $\bx_n$,}
				\STATE \textit{Standardization of Data Point:} Standardize $\bx_i \in \Real^p$ so that each component of $\bx_i$ has zero mean and variance one. We still use $\bx_i$ to denote the data point after standardization. 
				
				\textit{Remark.} By standardization, no component variable has undue influence on the results just because it has a large variance or absolute value. 
				
				\STATE \textit{Find the Best-matching Unit:} Compute the distance between $\bx_i$ and each representative $\bm_k$, and find the node whose representative yields the smallest distance to $\bx_i$. In other words, we solve the following optimization problem 
				\begin{align*}
					\minimize_{k \in \calK} \, \norm{\bx_i - \bm_k}_2^2, 
				\end{align*}
				and let the minimizer be $k_i^*$. The representative $\bm_{k_i^*}$ is declared as the ``winner'', and $k_i^*$ is referred to as the \emph{best-matching unit} (BMU) or \emph{winning node} for $\bx_i$. 
				
				\STATE \textit{Update the Representatives of Neighbors:} 	We update the representatives corresponding to $k_i^*$ and each of its $c$-grid neighbors so that each $\bm_k$, $k \in \calN_c \parens{k_i^*}$, is closer to $\bx_i$ through 
				\begin{align}\label{eq-update}
					\bm_k \quad \longleftarrow \quad \bm_k + \alpha \parens{\bx_i - \bm_k}, \qquad \text{ for all } k \in \calN_c \parens{k_i^*}, 
				\end{align}
				where $\alpha \in \parens{0, 1}$ is the learning rate. 
				
				% \textit{Remark.} We can set $\alpha = 0$ for all $k \notin \calN_c \parens{k_i^*}$ so that $\bm_k$'s with $k \notin \calN_c \parens{k_i^*}$ remain unchanged. 
				
				\STATE Repeat the preceding process for a large number of times. 
				
				\ENDFOR
				
			\end{algorithmic}
		\end{algorithm}
	\end{minipage}
	\vspace{10pt}
	
	\textit{Remark.} The effect of the update \eqref{eq-update} is to move the prototypes closer to the data, but also to maintain a smooth two-dimensional spatial relationship between the prototypes. 
	
	\item \textbf{Updating Representatives Using Distance-weighted Function:} A ``distance-weighted'' version of \eqref{eq-update} is 
	\begin{align*}
		\bm_k \quad \longleftarrow \quad \bm_k + \alpha h_{k}\parens{\norm{\bm_k - \bm_{k_i^*}}_2} \parens{\bx_i - \bm_k}, \qquad \text{ for all } k \in \calN_c \parens{k_i^*}, 
	\end{align*}
	where the neighborhood function $h_{k}$ depends upon how close the neighboring representatives are to $\bm_{k_i^*}$. The closer $\bm_k$ is to $\bm_{k_i^*}$, the more weights are given to $\bm_k$. 
	
	\textit{Example.} A popular choice of $h_{k}$ is the Gaussian kernel function given by 
	\begin{align*}
		h_k \parens{\norm{\bm_k - \bm_{k_i^*}}} = \exp \parens[\bigg]{\frac{\norm{\bm_k - \bm_{k_i^*}}_2^2}{2 \sigma^2}}
	\end{align*}
	where $\sigma > 0$ is the neighborhood radius. 
	
	\item \textbf{Effects of Threshold $c$:} If we take the threshold value $c$ to be so small that each neighborhood contains only a single point, we lose the dependencies between representatives. The SOM algorithm reduces to an on-line version of $K$-means clustering, where $K$ is the total number of nodes. 
	
	\item \textbf{Choices of $c$, $\alpha$ and $\sigma$:} During the course of running the SOM algorithm, all three parameters, $c$, $\alpha$ and $\sigma$, should be monotonically decreasing. 
	
\end{enumerate}

\printbibliography

\end{document}
