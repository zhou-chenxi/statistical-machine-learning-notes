\documentclass[12pt]{article}

\RequirePackage{amsmath}
\RequirePackage{amsthm}
\RequirePackage{amssymb}
\RequirePackage[mathscr]{eucal}
\RequirePackage{mathtools}
\RequirePackage{etoolbox}

%\usepackage{tgpagella}

\usepackage[red]{zhoucx-notation}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\renewcommand{\qedsymbol}{\hfill\rule{2mm}{2mm}}

\pagestyle{fancy}
\fancyhf{}
\setlength{\headheight}{15pt}
\rhead{\textsf{Chapter 19, Projection Pursuit Regression}}
\lhead{\textsf{Chenxi Zhou}}
\renewcommand{\headrulewidth}{1pt}
\cfoot{\thepage}

\newcommand{\titlebox}[4]{
\begin{tcolorbox}[colback = blue!5!white!95, colframe = blue!70!black
% colback = yellow!30!white, colframe = yellow!70!black 
]
  \noindent \textbf{ #1 } \hfill \textit{#2} 
  \begin{center}
  	 \LARGE{\textbf{#3}}
  \end{center}
\textbf{Chapter:} \textit{#4} \hfill \textbf{Prepared by:} \textit{Chenxi Zhou}
\end{tcolorbox}
}

\begin{document}

\titlebox{Notes on Statistical and Machine Learning}{}{Projection Pursuit Regression}{19}
\thispagestyle{plain}

\vspace{10pt}

This note is prepared based on \textit{Chapter 11, Neural Networks} in \textcite{Friedman2001-np}. 


\section*{I. Projection Pursuit Regression} 

\begin{enumerate}[label=\textbf{\arabic*.}]
	
	\item \textbf{Setup:} We assume that the input vector $\bx$ has $p$ components and the target variable is $y \in \Real$. Let $\bome_1, \bome_2, \cdots, \bome_M$ be $p$-dimensional unit vectors of unknown parameters. 
	
	\item \textbf{Model Specification:} The projection pursuit regression (PPR) model has the form 
	\begin{equation}\label{PPR}
		f \parens{\bx} = \sum_{m=1}^M g_m \parens{\bome_m^\top \bx}. 
	\end{equation}
	\begin{enumerate}
		\item This is an additive model in the derived features $v_m = \bome_m^\top \bx$. Here, the functions $g_m$ are \emph{unspecified} and are estimated along with the unknown directions $\bome_m$; 
		\item The function $\bx \mapsto g_m \parens{\bome^\top \bx}$ is called a \textit{ridge function} in $\Real^p$ and only varies in the direction defined by the vector $\omega_m$. 
		\item The scalars $v_m = \bome_m^\top \bx$ is the projection of $\bx$ onto the unit vector $\bome_m$. 
	\end{enumerate}
	
	\item \textbf{Comments on PPR Model:}
	\begin{enumerate}
		\item If $M$ is taken arbitrarily large, for appropriate choices of $g_m$, the PPR model \eqref{PPR} can approximate any continuous function in $\Real^p$ arbitrarily well. Such a class of models is called a \textit{universal approximator}. 
		\item \textit{Interpretation} of such a universal approximator is usually difficult. The PPR model is most useful for \textit{prediction}, and \emph{not} very useful for producing an understandable model for the data. 
		
		An exception is when $M = 1$, which is called \textit{single index model} in econometrics. 
	\end{enumerate}
	
	\item \textbf{Fitting the PPR Model:} To fit PPR model, given the training data $\sets{ \parens{\bx_i, y_i}}_{i=1}^n$, we minimize the following criterion 
	\begin{equation}\label{obj}
		\sum_{i=1}^n \bracks[\Bigg]{ y_i - \sum_{m=1}^M g_m \parens{\bome_m^\top \bx_i}}^2
	\end{equation}
	over functions $g_m$ and direction vectors $\bome_m$ for all $m = 1, \cdots, M$. 
	
	We consider the case when $M = 1$. 
	\begin{itemize}
		\item Given the direction vector $\bome$, we have the derived variables $v_i = \bome^\top \bx_i$. Then, the resulting problem is a one-dimensional smoothing problem and we can apply any smoother (e.g., smoothing spline) to obtain an estimate of $g$; 
		\item Given the function $g$, we wish to minimize with respect to the direction vector $\bome$. We can use the \textit{Gauss-Newton algorithm}, which is a quasi-Newton method where the Hessian matrix part involving the second-order derivative of $g$ is discarded. Letting $\bome^{\parens{\text{old}}}$ be the current estimate of $\bome$, by Taylor's expansion, we have 
		\begin{align*}
			g \parens{\bome^\top \bx_i} \approx g \parens{{\bome^{\parens{\text{old}}}}^\top \bx_i} + g' \parens{{\bome^{\parens{\text{old}}}}^\top \bx_i} \cdot \parens{\bome - {\bome^{\parens{\text{old}}}}^\top \bx_i}. 
		\end{align*}
		Plugging into the objective function \eqref{obj}, we have 
		\begin{align*}
			& \sum_{i=1}^n \parens[\big]{y_i - g \parens{\bome^\top \bx_i}}^2 \\ 
			\approx & \sum_{i=1}^n \parens[\Big]{y_i - g \parens{{\bome^{\parens{\text{old}}^\top}} \bx_i} - g' \parens{{\bome^{\parens{\text{old}}^\top}} \bx_i} \cdot \parens{\bome - {\bome^{\parens{\text{old}}}}}^\top \bx_i}^2 \\
			= & \sum_{i=1}^n \parens[\big]{g' \parens{{\bome^{\parens{\text{old}}}}^\top \bx_i}}^2 \bracks[\Bigg]{ \frac{y_i - g \parens{{\bome^{\parens{\text{old}}}}^\top \bx_i}}{g' \parens{{\bome^{\parens{\text{old}}}}^\top \bx_i}} - \parens{\bome - \bome^{\parens{\text{old}}}}^\top \bx_i}^2 \\ 
			= & \sum_{i=1}^n \parens[\big]{g' \parens{ {\bome^{\parens{\text{old}}}}^\top \bx_i}}^2 \bracks[\Bigg]{\parens[\bigg]{ {\bome^{\parens{\text{old}}}}^\top \bx_i +  \frac{y_i - g \parens{ {\bome^{\parens{\text{old}}}}^\top \bx_i} }{g' \parens{ {\bome^{\parens{\text{old}}}}^\top \bx_i}}} - \bome^\top \bx_i}^2. 
		\end{align*}
		To minimize the right-hand side, one can use the weighted least squares regression with the target 
		\begin{align*}
			{\bome^{\parens{\text{old}}}}^\top \bx_i +  \frac{y_i - g \parens{ {\bome^{\parens{\text{old}}}}^\top \bx_i} }{g' \parens{ {\bome^{\parens{\text{old}}}}^\top \bx_i}}
		\end{align*}
		on the input $\bx_i$, weights $\parens{g' \parens{ {\bome^{\parens{\text{old}}}}^\top \bx_i}}^2$ and no intercept. Then, we can obtain the the updated coefficient vector $\bome^{\parens{\text{new}}}$. 
		
		\item In this view, the estimation of the unknown function $g$ and parameter $\bome$ has two steps. These two steps are iterated until convergence.  

	\end{itemize}
	
	\textit{Remark.} If $M > 1$, the model can be built in a \textit{forward stage-wise} manner, adding a pair $\parens{\bome_m, g_m}$ at each stage. 
	
	\item \textbf{Implementation Details:}
	\begin{enumerate}
		\item It is more convenient to use local regression and smoothing splines to estimate $g$; 
		\item The number of terms $M$ is usually estimated as part of the forward stage-wise strategy. The model building stops when the next term does \emph{not} appreciably improve the fit of the model. Cross-validation can also be used to determine $M$. 
	\end{enumerate}

\end{enumerate}


\printbibliography

\end{document}
