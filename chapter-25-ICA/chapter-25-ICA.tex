\documentclass[12pt]{article}

\RequirePackage{amsmath}
\RequirePackage{amsthm}
\RequirePackage{amssymb}
\RequirePackage[mathscr]{eucal}
\RequirePackage{mathtools}
\RequirePackage{etoolbox}
\usepackage[red]{zhoucx-notation}

% \usepackage{subcaption}
\usepackage{lipsum}

\geometry{letterpaper, top = 1in, bottom = 1in, left = 1in, right = 1in}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\renewcommand{\qedsymbol}{\hfill\rule{2mm}{2mm}}

\pagestyle{fancy}
\fancyhf{}
\setlength{\headheight}{15pt}
\rhead{\textsf{Chapter 25, Independent Component Analysis}}
\lhead{\textsf{Chenxi Zhou}}
\renewcommand{\headrulewidth}{1pt}
\cfoot{\thepage}

\newcommand{\titlebox}[4]{
\begin{tcolorbox}[colback = blue!5!white!95, colframe = blue!70!black
% colback = yellow!30!white, colframe = yellow!70!black 
]
  \noindent \textbf{ #1 } \hfill \textit{#2} 
  \begin{center}
  	 \LARGE{\textbf{#3}}
  \end{center}
\textbf{Chapter:} \textit{\text{#4}} \hfill \textbf{Prepared by:} \textit{Chenxi Zhou}
\end{tcolorbox}
}

\begin{document}

\thispagestyle{plain}
\titlebox{Notes on Statistical and Machine Learning}{}{Independent Component Analysis}{25}

\vspace{10pt} 

This note is prepared based on 
\begin{itemize}
	\item \textit{Chapter 15, Latent Variable Models for Blind Source Separation} in \textcite{Izenman2009-jk}, and 
	\item \textit{Chapter 14, Unsupervised Learning} in \textcite{Friedman2001-np}. 
\end{itemize}


\section*{I. Introduction}

\begin{enumerate}[label=\textbf{\arabic*.}]

	\item \textbf{Blind Source Separation Problem:} The \textit{blind source separation} (BSS) problem involves decomposing an unknown mixture of non-Gaussian signals into its independent component signals. 
	
	\item \textbf{Overview:} \textit{Independent component analysis} (ICA) is a multivariate statistical technique that seeks to uncover hidden variables in high-dimensional data and solve the blind source separation problem. 
	
	\item \textbf{Assumption:} 
	\begin{enumerate}
		\item The ICA model is a linear mixture of an \emph{unknown} number of \emph{unknown} hidden source variables, where the mixing coefficients are also \emph{unknown}; 
		\item The hidden variables are mutually independent; 
		\item The hidden variables are (with at most one exception) non-Gaussian. 
	\end{enumerate}
	
	\item \textbf{Setup:} Let $X = \parens{X_1, \cdots, X_p}^\top$ be a $p$-dimensional random vector with mean $\E \bracks{X} = \bmu \in \Real^p$ and covariance matrix $\bSigma_{XX} \in \Real^{p \times p}$. 
	
	\item \textbf{Preprocessing:} We 
	\begin{enumerate}
		\item center $X$ so that its components have zero mean, and 
		\item whiten the result so that its components are uncorrelated and have unit variances. 
	\end{enumerate}
	Let $\bSigma_{XX} = \bU \bLambda \bU^\top$ be the spectral decomposition of $\bSigma_{XX}$, where $\bLambda \in \Real^{p \times p}$ is a diagonal matrix with the eigenvalues of $\bSigma_{XX}$ on the diagonal. Since $\bSigma_{XX} \succeq \boldzero_{p \times p}$, all diagonal elements of $\bLambda$ are nonnegative. 
	\begin{enumerate}
		\item Assume both $\bmu$ and $\bSigma_{XX}$ are known. Then, we can achieve the goal of preprocessing by performing 
		\begin{align}
			X \longleftarrow \bLambda^{-\frac{1}{2}} \bU^\top \parens{X - \bmu}. 
		\end{align}
		\item Typically, $\bmu$ and $\bSigma_{XX}$ are unknown. Let $\bx_1, \cdots, \bx_n$ be $n$ i.i.d observations from $X$. We estimate $\bmu$ and $\bSigma_{XX}$ by 
		\begin{align*}
			\bar{\bx} := \frac{1}{n} \sum_{i=1}^n \bx_i, \qquad \text{ and } \qquad \widehat{\bSigma}_{XX} := \frac{1}{n} \sum_{i=1}^n \parens{\bx_i - \bar{\bx}} \parens{\bx_i - \bar{\bx}}^\top, 
		\end{align*}
		respectively. Let $\widehat{\bSigma}_{XX} = \widehat{\bU} \widehat{\bLambda} \widehat{\bU}^\top$ be the spectral decomposition of $\widehat{\bSigma}_{XX}$. We can preprocess each $\bx_i$ as 
		\begin{align*}
			\bx_i \longleftarrow \widehat{\bLambda}^{-\frac{1}{2}} \widehat{\bU}^\top \parens{\bx_i - \bar{\bx}}, \qquad \text{ for all } i = 1, \cdots, n. 
		\end{align*}
	\end{enumerate}

\end{enumerate}


\section*{II. The General ICA Problem}

\begin{enumerate}[label=\textbf{\arabic*.}]
	
	\item \textbf{ICA Model:} The \textit{ICA model} assumes that the $p$-dimensional random vector $X$ is generated by 
	\begin{align}\label{eq-ica-model}
		X = f \parens{S} + \varepsilon, 
	\end{align}
	where $S = \parens{S_1, S_2, \cdots, S_m}^\top \in \Real^m$ is an unobserved random vector of sources. We assume 
	\begin{enumerate}
		\item components $S_1, S_2, \cdots, S_m$ are independent latent variables, 
		\item $\E \bracks{S_j} = 0$ for all $j = 1, \cdots, m$, and $\var \bracks{S} = \bI_m$, 
		\item $f: \Real^m \to \Real^p$ is an unknown \emph{mixing function}, and 
		\item $\varepsilon \in \Real^p$ is an additive component with $\E \bracks{\varepsilon} = \boldzero_{p}$ that represents measurement noise and any other type of variability that \emph{cannot} be directly attributed to the sources. 
	\end{enumerate}
	
	\item \textbf{Goal:} The goal is to invert $f$ and estimate $S$. 
	
	\item \textbf{Ill-posedness of the Problem:} With the setup so far, the problem is ill-posed, and needs some additional constraints or regularization to achieve the desired goal. Examples include the following: 
	\begin{enumerate}
		\item If we let $f \parens{S} = \bA S$, where $\bA$ is a ``mixing'' matrix, then \eqref{eq-ica-model} is a \emph{linear ICA model}; if $f$ is a nonlinear function, \eqref{eq-ica-model} is a \emph{nonlinear ICA model}. 
		\item If we require $\varepsilon = \boldzero_p$, i.e., there is no random noise so that all noise in the model is associated with the components of $S$, we obtain the \textit{noiseless ICA model}. 
	\end{enumerate}	
	
\end{enumerate}


\section*{III. Linear Noiseless ICA}

\begin{enumerate}[label=\textbf{\arabic*.}]

	\item \textbf{Setup:} We consider the ICA model that has the linear mixing and has no additive noise. In this scenario, $X$ is modeled deterministically as 
	\begin{align*}
		X = \bA S, 
	\end{align*}
	where $S = \parens{S_1, S_2, \cdots, S_m}^\top$ is a latent $m$-dimensional random vector of independent source components, and $\bA \in \Real^{p \times m}$ is a full-rank mixing matrix of unknown parameters. We require $m \le p$. 
	
	\item \textbf{No Solution Exists When $S$ Has Independent Gaussian Component:} Suppose $m = p$. The resulting linear noiseless ICA model, $X = \bA S$, can \emph{only} be solved if independent components of $S$ are \emph{not} Gaussian. 
	
	Suppose the contrary that $S_1, S_2, \cdots, S_m$ are independent and Gaussian with mean 0 and variance 1. The joint density function is 
	\begin{align*}
		q_S \parens{\bs} := \frac{1}{\parens{2\pi}^{\frac{p}{2}}} \exp \parens[\bigg]{ - \frac{1}{2} \sum_{j=1}^p s_j^2}. 
	\end{align*}
	Recall that $X$, after the preprocessing, has $\var \bracks{X} = \bI_p = \var \bracks{S}$. Then, the mixing matrix $\bA$ must satisfy 
	\begin{align*}
		\bI_p = \bSigma_{XX} = \bA \bA^\top; 
	\end{align*}
	in other words, we have $\bA = \bA^{-1}$, i.e., $\bA$ is orthogonal. It follows that the density function of $X = \bA S$ is 
	\begin{align*}
		q_X \parens{\bx} := & \, \frac{1}{\parens{2 \pi}^{\frac{p}{2}}} \exp \parens[\bigg]{-\frac{1}{2} \norm{\bA^\top \bx}_2^2} \abs{ \det \parens{\bA^\top }} \\ 
		= & \, \frac{1}{\parens{2 \pi}^{\frac{p}{2}}} \exp \parens[\bigg]{-\frac{1}{2} \norm{\bx}_2^2}. 
	\end{align*}
	Thus, the density of $X$ reduces to that of $S$, and the orthogonal mixing matrix $\bA$ \emph{cannot} be identified for independent Gaussian sources. 
	
	\textit{Remedy:} We need to require that, with the exception of at most one component, the remaining independent source components cannot be Gaussian distributed. 
	
	\item \textbf{Goal:} Given $n$ i.i.d observations on $X$, the ICA problem attempts to estimate $\bA$ and, hence, recover $S$. 

	\item \textbf{Solution with a Given $\bA$:} For a given $\bA$ with full rank, there exists a un-mixing matrix $\bW$ such that the sources can be recovered exactly from the observed X by 
	\begin{align}
		S = \bW X, 
	\end{align}
	where $\bW = \parens{\bA^\top \bA}^{-1} \bA^{\top}$. 
	
	\textit{Special Cases:} If the number of independent sources is equal to the number of measurements, i.e., $m = p$, we have $\bW = \bA^{-1}$. If $X$ has been centered and sphered, then the square mixing matrix $\bA$ is orthogonal, and so $\bW = \bA^{\top}$. 
	
	\item \textbf{Solution When $\bA$ is Unknown:} In practice, $\bA$ is \emph{unknown} and the goal is to estimate $\bW$ and the source components based solely upon the observations of $X$. Given an estimate $\widehat{\bW} = \parens{\hat{\bw}_1, \cdots, \hat{\bw}_m}^\top \in \Real^{m \times p}$ of the un-mixing matrix $\bW$, the source component vector $S$ is approximated by 
	\begin{align*}
		Y = \widehat{\bW} X, 
	\end{align*}
	where the elements $Y_1 := \hat{\bw}_1^\top X$, $Y_2 := \hat{\bw}_2^\top X$, $\cdots$, $Y_m := \hat{\bw}_m^\top X$ are taken to be statistically independent and as non-Gaussian as possible. 

\end{enumerate}


\section*{IV. Non-polynomial Based Approximation}

\begin{enumerate}[label=\textbf{\arabic*.}]

	\item \textbf{Goal:} In this section, we aim to achieve the following two goals: 
	\begin{enumerate}
		\item we approximate the density function of a random variable $Y$ under certain moment constraints and the assumption that $\log p_Y$ is a sum of non-polynomial functions, 
		\item under this approximation, we derive the formula for the \emph{entropy} 
		\begin{align}
			H \parens{Y} = - \int p_Y \parens{y} \log p_Y \parens{y} \diff y, 
		\end{align}
		where $p_Y$ is the density function of the random variable $Y$, and the \emph{negentropy}  
		\begin{align}
			J \parens{Y} = H \parens{Z} - H \parens{Y}, 
		\end{align}
		where $Z$ is a Gaussian random variable with the same variance as $Y$. 
	\end{enumerate}
	Achieving these two goals helps us to derive the FastICA algorithm in the next section. 
	
	\item \textbf{Setup:} Suppose that $g_j$, for $j = 1, 2, \cdots, m$, are distinct non-polynomial functions that 
	\begin{enumerate}
		\item form an orthonormal system with respect to the standard Gaussian density function $\varphi$, i.e., 
		\begin{align}\label{eq-ica-ortho1}
			\int \varphi \parens{y} g_i \parens{y} g_j \parens{y} \diff y = \delta_{i, j}, 
		\end{align}
		where $\delta_{i, j} = 1$ if and only if $i = j$ and $\delta_{i, j} = 0$ otherwise, and 
		\item are orthogonal to all polynomials of degrees up to 2, i.e., for all $j = 1, 2, \cdots, m$, the following equations hold
		\begin{equation}\label{eq-ica-ortho2}
			\begin{aligned}
				\int \varphi \parens{y} g_j \parens{y} \diff y = 0, \qquad
				\int \varphi \parens{y} g_j \parens{y} y \diff y = 0, \qquad
				\int \varphi \parens{y} g_j \parens{y} y^2 \diff y = 0. 
			\end{aligned}
		\end{equation}
	\end{enumerate}
	\textit{Remark.} We can find functions $g_1, g_2, \cdots, g_m$ that satisfy the orthogonality conditions \eqref{eq-ica-ortho1} and \eqref{eq-ica-ortho2} using the Gram-Schmidt process. 
	
	\item \textbf{Assumptions:} 
	\begin{enumerate}
		\item The expectations of $g_j \parens{Y}$, for $j = 1, 2, \cdots, m$, are given by the following equations 
		\begin{align}\label{eq-ica-constr1}
			\E \bracks{g_j \parens{Y}} = \int g_j \parens{y} p_Y \parens{y} \diff y = c_j, \qquad \text{ for all } j = 1, \cdots, m; 
		\end{align}
		\item $Y$ has the zero mean and unit variance, i.e., 
		\begin{align}\label{eq-ica-constr2}
			\E \bracks{Y} = 0, \qquad \text{ and } \qquad \var \bracks{Y} = 1. 
		\end{align}
	\end{enumerate}
	
	\item \textbf{Density Function of $Y$:} If the probability density $p_Y$ satisfies the constraints \eqref{eq-ica-ortho1} - \eqref{eq-ica-constr2} and also has the \emph{largest} entropy among all such densities, then $p_Y$ must be of the form 
	\begin{align}
		p_Y \parens{y} = A \exp \parens[\Bigg]{\sum_{j=1}^{m+2} a_j g_j \parens{y}}, 
	\end{align}
	where we let $g_{m+1} \parens{y} = y$ and $g_{m+2} \parens{y} = y^2$, $A$ is the normalizing constant to ensure that $p_Y$ is a valid probability density function, and $a_1, \cdots, a_{m+2}$ are chosen so that \eqref{eq-ica-constr1} and \eqref{eq-ica-constr2} are satisfied. 
	
	\item \textbf{Approximate Maximum Entropy Density:} We further require $p_Y$ to be close to $\varphi$, where $\varphi$ is the density function of the standard normal distribution, then 
	\begin{align*}
		p_Y \parens{y} = & \, A \exp \parens[\Bigg]{-\frac{y^2}{2} + a_{m+1} y + \parens[\bigg]{a_{m+2} + \frac{1}{2}} y^2 + \sum_{j=1}^m a_j g_j \parens{y}} \\ 
		\approx & \, \widetilde{A} \varphi \parens{y} \parens[\Bigg]{ 1 + a_{m+1} y + \parens[\bigg]{a_{m+2} + \frac{1}{2}} y^2 + \sum_{j=1}^m a_j g_j \parens{y}} \\ 
		=: & \, \tilde{p}_Y \parens{y}, 
	\end{align*}
	where $\widetilde{A} = \sqrt{2 \pi} A$ and we use the approximation $e^{\varepsilon} \approx 1 + \varepsilon$ in the last step. 
	
	Under our assumptions earlier, $\tilde{p}_Y$ must satisfy the following constraints 
	\begin{align*}
		1 = & \, \int \tilde{p}_Y \parens{y} \diff y = \widetilde{A} \parens[\bigg]{1 + a_{m+2} + \frac{1}{2}}, \\ 
		0 = & \, \E \bracks{Y} = \int \tilde{p}_Y \parens{y} y \diff y = \widetilde{A} a_{m+1}, \\ 
		1 = & \, \E \bracks{Y^2} = \int \tilde{p}_Y \parens{y} y^2 \diff y = \widetilde{A} \parens[\Bigg]{1 + 3 \parens[\bigg]{a_{m+2} + \frac{1}{2}}}, \\ 
		c_j = & \, \int \tilde{p}_Y \parens{y} g_j \parens{y} \diff y = \widetilde{A} a_j, \qquad \text{ for all } j = 1, \cdots, m. 
	\end{align*}
	From the equations above, we can solve 
	\begin{align*}
		a_j = & \, c_j, \qquad \text{ for all } j = 1, \cdots, m, \\ 
		a_{m+1} = & \, 0, \qquad a_{m+2} = -\frac{1}{2}, \qquad \widetilde{A} = 1. 
	\end{align*}
	It follows that the resulting density function $\tilde{p}_Y$ is 
	\begin{align}
		\tilde{p}_Y \parens{y} = \varphi \parens{y} \parens[\Bigg]{1 + \sum_{j=1}^m c_j g_j \parens{y}}, 
	\end{align}
	which is referred to as the \emph{approximate maximum entropy density}. 
	
	\item \textbf{Entropy of $\tilde{p}_Y$:} Using the definition of the entropy, we have 
	\begin{align*}
		H \parens{Y} = & \, - \int p_Y \parens{t} \log p_Y \parens{y} \diff y \\ 
		\approx & \, - \int \tilde{p}_Y \parens{t} \log \tilde{p}_Y \parens{y} \diff y \\ 
		= & \, - \int \varphi \parens{y} \parens[\Bigg]{1 + \sum_{j=1}^m c_j g_j \parens{y}} \log \parens[\Bigg]{\varphi \parens{y} \parens[\Bigg]{1 + \sum_{j=1}^m c_j g_j \parens{y}}} \\ 
		\approx & \, - \int \varphi \parens{y} \log \varphi \parens{y} \diff y - \sum_{j=1}^m c_j \int \varphi \parens{y} g_j \parens{y} \log \varphi \parens{y} \diff y \\ 
		& \qquad \qquad - \int \varphi \parens{y} \parens[\Bigg]{1 + \sum_{j=1}^m c_j g_j \parens{y}} \log \parens[\Bigg]{1 + \sum_{j=1}^m c_j g_j \parens{y}} \diff y \\ 
		= & \, H \parens{Z} - \sum_{j=1}^m c_j \int \varphi \parens{y} g_j \parens{y} \log \varphi \parens{y} \diff y - \sum_{j=1}^m c_j \int \varphi \parens{y} g_j \parens{y} \diff y \\ 
		& \qquad \qquad - \frac{1}{2} \sum_{j=1}^m c_j^2 \int \varphi \parens{y} g_j^2 \parens{y} \diff y - o \parens[\Bigg]{\sum_{j=1}^m c_j^2 \int \varphi \parens{y} g_j^2 \parens{y} \diff y}  \\ 
		= & \, H \parens{Z} - \frac{1}{2} \sum_{j=1}^m c_i^2 + o \parens[\Bigg]{\sum_{j=1}^m c_j^2}, 
	\end{align*}
	where $Z$ is the standard normal random variable, we use the conditions \eqref{eq-ica-ortho1} and \eqref{eq-ica-ortho2}, and the expansion $\parens{1 + \varepsilon} \log \parens{1 + \varepsilon} \approx \varepsilon + \frac{1}{2} \varepsilon^2 + o \parens{\varepsilon^2}$ for small $\varepsilon$, and $\log \varphi \parens{y} = - \frac{1}{2} \log \parens{2 \pi} - \frac{1}{2} y^2$. 
	
	Based on the calculation above, we have 
	\begin{align*}
		H \parens{Z} - H \parens{Y} \approx J_m \parens{Y} := \frac{1}{2} \sum_{j=1}^m \parens[\big]{\E \bracks{g_j \parens{Y}}}^2. 
	\end{align*}
	
	\textit{Remark.} Up to this point, what remains is to choose an appropriate value of $m$ and appropriate basis functions $g_1, g_2, \cdots, g_m$. 
	
	\item \textbf{Choices of $m$:}  
	\begin{enumerate}
		\item If $m=2$, we can make 
		\begin{enumerate}
			\item $g_1$ an odd function, reflecting symmetry vs. asymmetry, and 
			\item $g_2$ an even function, reflecting sub-Gaussian (negative kurtosis) vs. super-Gaussian (positive kurtosis) distributions. 
		\end{enumerate}
		In this case, we have 
		\begin{align}\label{eq-obj-2}
			J_2 \parens{Y} = \beta_1 \parens[\big]{\E \bracks{g_1 \parens{Y}}}^2 + \beta_2 \parens[\big]{\E \bracks{g_2 \parens{Y}} - \E \bracks{g_2 \parens{Z}}}^2, 
		\end{align}
		where $\beta_1 > 0$ and $\beta_2 > 0$. 
		
		\item If $m = 1$, we have 
		\begin{align}\label{eq-obj-1}
			J_1 \parens{Y} = \beta \parens[\big]{\E \bracks{g_1 \parens{Y}} - \E \bracks{g_1 \parens{Z}}}^2, 
		\end{align}
		where $\beta > 0$. 
	\end{enumerate}
	
	\item \textbf{Choices of $\sets{g_j}_{j=1}^m$:}
	\begin{enumerate}
		\item logcosh function: $g \parens{y} = \frac{1}{\alpha} \log \cosh \parens{\alpha y}$, where $\alpha \in \bracks{1, 2}$; 
		\item exp function: $g \parens{y} = - \exp \parens{- \frac{1}{2} y^2}$. 
	\end{enumerate}	

\end{enumerate}


\section*{V. FastICA Algorithm for a Single Source Component}

\begin{enumerate}[label=\textbf{\arabic*.}]

	\item \textbf{Goal:} Consider a single ($m = 1$) source component $Y = \bw^\top X$, where the $p$-vector $\bw$ represents a direction for a one-dimensional projection. We wish to find $\bw$ that maximizes the approximation \eqref{eq-obj-1} subject to the constraint $\E \bracks{ \parens{\bw^\top X}^2 } = \norm{\bw}_2^2 = 1$ on the projection. 
	
	\textit{Remark.} In the criterion above, $\bw$ is to be the direction that makes the density of the one-dimensional projection $Y = \bw^\top X$ as \emph{far away} from the Gaussian density as possible. 
	
	\item \textbf{Problem Formulation:} We solve the following optimization problem 
	\begin{align*}
		\maximize & \, \ J_1 \parens{Y} = \beta \parens[\big]{\E \bracks{g_1 \parens{Y}} - \E \bracks{g_1 \parens{Z}}}^2 \\ 
		\text{subject to } & \, \norm{\bw}_2^2 = 1. 
	\end{align*}
	Because the maxima of $J_1 \parens{\bw^\top X}$ are typically obtained at certain maxima of $\E \bracks{g_1 \parens{\bw^\top X}}$, we work with 
	\begin{align}\label{eq-fastica-obj}
		F \parens{\bw} := \E \bracks{g_1 \parens{\bw^\top X}} - \frac{\lambda}{2} \parens{\norm{\bw}_2^2 - 1}, 
	\end{align}
	where $\lambda > 0$ is the Lagrangian multiplier. 
	
	\item \textbf{Newtow-Raphson Algorithm:} We apply the Newton-Raphson algorithm to maximize \eqref{eq-fastica-obj}. The iterations are 
	\begin{align}
		\bw \longleftarrow \bw - \parens[\bigg]{\frac{\partial^2 F \parens{\bw}}{\partial \bw \partial \bw^\top}}^{-1} \parens[\bigg]{\frac{\partial F \parens{\bw}}{\partial \bw}}. 
	\end{align}
	Note that 
	\begin{align*}
		\frac{\partial F \parens{\bw}}{\partial \bw} = \E \bracks{X g' \parens{\bw^\top X}} - \lambda \bw. 
	\end{align*}
	Any stationary point must satisfy $\frac{\partial F \parens{\bw}}{\partial \bw} = \boldzero_p$. Premultiplying both sides of the preceding equation by $\bw$ yields 
	\begin{align*}
		\lambda = \E \bracks{\bw^\top X g' \parens{\bw^\top X}}. 
	\end{align*}
	In addition, we have 
	\begin{align*}
		\frac{\partial^2 F \parens{\bw}}{\partial \bw \partial \bw^\top} = & \, \E \bracks{X X^\top g'' \parens{\bw^\top X}} - \lambda \bI_p \\ 
		\approx & \, \E \bracks{X X^\top} \E \bracks{ g'' \parens{\bw^\top X}} - \lambda \bI_p \\ 
		= & \, \parens[\big]{\E \bracks{ g'' \parens{\bw^\top X}} - \lambda} \bI_p, 
	\end{align*}
	where we use the fact that $X$ has been sphered. 
	
	It follows that the iterations of $\bw$ are 
	\begin{align}\label{eq-fastica-update}
		\bw \longleftarrow \bw - \frac{\E \bracks{X g' \parens{\bw^\top X}} - \lambda \bw}{\E \bracks{ g'' \parens{\bw^\top X}} - \lambda}. 
	\end{align}
	In practice, the expectation can be approximated using the sample average. 
	
	\item \textbf{Alternative Expression of \eqref{eq-fastica-update}:} The $k$-th iterate of \eqref{eq-fastica-update} is 
	\begin{align*}
		\bw_k = \bw_{k-1} - \frac{\E \bracks{X g' \parens{\bw_{k-1}^\top X}} - \lambda \bw_{k-1}}{\E \bracks{ g'' \parens{\bw_{k-1}^\top X}} - \lambda}. 
	\end{align*}
	Multiplying both sides by $\E \bracks{ g'' \parens{\bw_{k-1}^\top X}} - \lambda$ and rearranging terms yields 
	\begin{align*}
		\bw_k \parens{\lambda - \E \bracks{ g'' \parens{\bw_{k-1}^\top X}}} = \E \bracks{X g' \parens{\bw_{k-1}^\top X}} - \bw_{k-1} \E \bracks{ g'' \parens{\bw_{k-1}^\top X}}. 
	\end{align*}
	Since we divide $\bw_k$ by its norm $\norm{\bw_k}$ at each step, the factor $\parens{\lambda - \E \bracks{ g'' \parens{\bw_{k-1}^\top X}}}$ on the left-hand side is \emph{not} necessary, and the update equation at the $k$-th iterate becomes 
	\begin{align*}
		\bw_k = \E \bracks{X g' \parens{\bw_{k-1}^\top X}} - \bw_{k-1} \E \bracks{ g'' \parens{\bw_{k-1}^\top X}}. 
	\end{align*}
	
	\item \textbf{Convergence Criterion:} The values of $\bw$ can change \emph{substantially} from iteration to iteration; this is because the ICA model \emph{cannot} determine the sign of $\bw$, so that $-\bw$ and $\bw$ become equivalent and define the same direction. 
	
	Hence, ``convergence'' of the FastICA algorithm is taken to mean that successive iterative values of $\bw$ are oriented in the same direction, i.e., the inner product between two iterations of $\bw$ is very close to 1. 
	
	\item \textbf{Complete FastICA Algorithm:}
	
	\begin{minipage}{\linewidth}
		\begin{algorithm}[H]
		\caption{FactICA Algorithm for a Single Source Component}\label{algo-fastica-single}
		\begin{algorithmic}[1]
		\STATE Center and whiten the data to give $X$; 
		\STATE Choose an initial version of the $p$-vector $\bw$ with unit norm; 
		\STATE Choose $g$ to be any non-quadratic density with the first and second partial derivatives $g'$ and $g''$, respectively. 
		\STATE Let 
		\begin{align*}
			\bw \longleftarrow \E \bracks{X g' \parens{\bw^\top X}} - \bw \E \bracks{ g'' \parens{\bw^\top X}}. 
		\end{align*}
		In practice, the expectations are estimated using sample averages. 
		\STATE Let $\bw \leftarrow \bw / \norm{\bw}_2$; 
		\STATE Iterate between steps 4 and 5. Stop when convergence is attained. 
		\end{algorithmic}
		\end{algorithm}
	\end{minipage}


\end{enumerate}


\section*{VI. FastICA Algorithm for Multiple Source Components}

\begin{enumerate}[label=\textbf{\arabic*.}]

	\item \textbf{Goal:} Extract multiple independent projections of $X$. 
	
	\item \textbf{Method 1 --- Deflation Method:} A single component that is orthogonal to \emph{all} previously found components (using the Gram-Schmidt process), and then the resulting new component is normalized. 
	
	\begin{minipage}{\linewidth}
		\begin{algorithm}[H]
		\caption{FactICA Algorithm for Multiple Source Components (Deflation Method)}\label{algo-fastica-multiple-deflation}
		\begin{algorithmic}[1]
		\STATE Center and whiten the data to give $X$; 
		\STATE Decide on the number, $m$, of independent components to be extracted; 
		\STATE For $j = 1, 2, \cdots, m$, 
		\begin{enumerate}
			\item Initialize (e.g., randomly) the $p$-vector $\bw_j$ to have unit norm; 
			\item Let 
			\begin{align*}
				\bw_j \longleftarrow \E \bracks{X g' \parens{\bw_{j}^\top X}} - \bw_{j} \E \bracks{ g'' \parens{\bw_{j}^\top X}}. 
			\end{align*}
			be the FastICA single component update for $\bw_j$. In practice, the expectations are estimated using sample averages; 
			\item Use the Gram-Schmidt process to orthogonalize $\bw_j$ with respect to the previously chosen $\bw_1, \cdots, \bw_{j-1}$ as 
			\begin{align*}
				\bw_j \longleftarrow \bw_j - \sum_{k=1}^{j-1} \parens{\bw_j^\top \bw_k} \bw_k; 
			\end{align*}
			\item Let $\bw_j \leftarrow \bw_j / \norm{\bw_j}_2$; 
			\item Iterate $\bw_j$ until convergence. 
		\end{enumerate}
		
		\STATE Set $j \leftarrow j+1$. If $j \le m$, return to Step 3. 
		\end{algorithmic}
		\end{algorithm}
	\end{minipage}
	
	\item \textbf{Method 2 --- Parallel Method:} The single component routine is carried out \emph{in parallel} for each independent component to be extracted, and then a symmetric orthogonalization is carried out on all components simultaneously. 
	
	\begin{minipage}{\linewidth}
		\begin{algorithm}[H]
		\caption{FactICA Algorithm for Multiple Source Components (Parallel Method)}\label{algo-fastica-multiple-parallel}
		\begin{algorithmic}[1]
		\STATE Center and whiten the data to give $X$; 
		\STATE Decide on the number, $M$, of independent components to be extracted; 
		\STATE Initialize (e.g., randomly) the $p$-vectors $\bw_1, \cdots, \bw_m$, each to have unit norm. Let $\bW = \parens{\bw_1, \cdots, \bw_m}^\top$; 
		\STATE Carry out a symmetric orthogonalization of $\bW$ by 
		\begin{align*}
			\bW \longleftarrow \parens{\bW \bW^\top}^{-\frac{1}{2}} \bW; 
		\end{align*}
		\STATE For each $j = 1, 2, \cdots, m$, let 
		\begin{align*}
			\bw_j \longleftarrow \E \bracks{X g' \parens{\bw_{j}^\top X}} - \bw_j \E \bracks{ g'' \parens{\bw_{j}^\top X}}. 
		\end{align*}
		be the FastICA single-component update for $\bw_j$. In practice, the expectations are estimated using sample averages; 
		\STATE Carry out another symmetric orthogonalization of $\bW$; 
		\STATE If convergence has not occurred, return to Step 5. 
		\end{algorithmic}
		\end{algorithm}
	\end{minipage}
	
	\item \textbf{Comparison of the Deflation and Parallel Methods:} 
	\begin{enumerate}
		\item The deflation method extracts independent components sequentially one at a time, whereas 
		\item the parallel method extracts all the independent components at the same time.  
	\end{enumerate}
\end{enumerate}


\section*{VII. Maximum Likelihood ICA}

\begin{enumerate}[label=\textbf{\arabic*.}]

	\item \textbf{Main Idea:} Specify a parametric distribution, $p_S$, for the latent source variables $S$ and then apply the maximum-likelihood (ML) method to estimate the parameters of that distribution. 

	\item \textbf{Setup:} We only consider the square mixing case (i.e., $m = p$) and the linear mixing case. 
	
	\item \textbf{Density Functions of $X$:} Let $p_S$ be the density function of $S$. Since $X = \bA S$, where $\bA \in \Real^{p \times p}$ is nonsingular, we let $\bW = \bA^{-1}$ and the density function of $X$ is 
	\begin{align*}
		p_X \parens{\bx} = \abs{\det \parens{\bW}} p_S \parens{\bs}. 
	\end{align*}
	Since the sources are assumed to be independent, we have 
	\begin{align}
		p_X \parens{\bx} = \abs{\det \parens{\bW}} \prod_{j=1}^m p_{S_j} \parens{\bw_j^\top \bx}, 
	\end{align}
	where $p_{S_j}$ is the density of $S_j$ and $\bw_j^\top$ is the $j$-th row of $\bW$. 
	
	\item \textbf{Log-likelihood Function:} Given $n$ i.i.d. observations, $\bx_1, \cdots, \bx_n$, the average log-likelihood function for $\bW$ is 
	\begin{align}\label{eq-ica-log-likelihood}
		L \parens{\bW} := \log \abs{\det \parens{\bW}} + \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^m \log p_{S_j} \parens{\bw_j^\top \bx_i}. 
	\end{align}
	
	\item \textbf{Algorithm:} We derive a fixed-point algorithm that maximizes \eqref{eq-ica-log-likelihood} numerically. Note that 
	\begin{align}
		\frac{\partial L \parens{\bW}}{\partial \bW} = & \, \parens{\bW^\top}^{-1} + \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^m \frac{\partial \log p_{S_j} \parens{\bw_j^\top \bx_i}}{\partial \bw_j} \nonumber \\ 
		= & \, \parens{\bW^\top}^{-1} + \frac{1}{n} \sum_{i=1}^n \bg \parens{\bW \bx_i} \bx_i^\top, 
	\end{align}
	where 
	\begin{align*}
		\bg \parens{\bW \bx} = & \, \parens[\big]{g_1 \parens{\bw_1^\top \bx}, g_2 \parens{\bw_2^\top \bx}, \cdots, g_m \parens{\bw_m^\top \bx}}, \\ 
		g_j \parens{\bw_j^\top \bx} = & \, \frac{p_{S_j}' \parens{\bw_j^\top \bx}}{p_{S_j} \parens{\bw_j^\top \bx}}. 
	\end{align*}
	The update for the $k$-th iteration of $\bW$ is 
	\begin{align}\label{eq-mlica-update}
		\bW_k = \bW_{k-1} - \alpha \frac{\partial L \parens{\bW}}{\partial \bW}\bigg\vert_{\bW = \bW_{k-1}}, 
	\end{align}
	where $\alpha > 0$ is the step size. 
	
	Set $\Delta \bW = \bW_k - \bW_{k-1}$. Then, we can rewrite \eqref{eq-mlica-update} as 
	\begin{align*}
		\Delta \bW \propto \parens{\bW^\top}^{-1} + \E_{\widehat{F}_n} \bracks{\bg \parens{\bW X} X^\top}, 
	\end{align*}
	where $\E_{\widehat{F}_n}$ denotes the sample average. Post-multiplying the right-hand side of the preceding equation by $\bW^\top \bW$ gives the \emph{fixed-point algorithm}  
	\begin{align}\label{eq-ml-fixed-point-algo}
		\bW \longleftarrow \bW + \alpha_0 \parens{\bI_m + \E_{\widehat{F}_n} \bracks{\bg \parens{\bW X} X^\top \bW^\top} } \bW, 
	\end{align}
	where $\alpha_0 > 0$ is the step size which may be reduced in size until convergence. 
	
	\textit{Remark.} The modification above produces an algorithm that avoids the matrix inversions in \eqref{eq-mlica-update} and speeds up convergence considerably. 

\end{enumerate}


\section*{VIII. Product Density ICA}

\begin{enumerate}[label=\textbf{\arabic*.}]
	
	\item \textbf{Goal:} This section presents the \emph{product density ICA}, abbreviated as ProDenICA, which has a similar flavor as the maximum likelihood ICA presented in the preceding section. 
	
	\item \textbf{Setup:} We only consider the square mixing case (i.e., $m = p$) and the linear mixing case. 
	
	\item \textbf{Tilted Gaussian Density Function:} Since components of $S = \parens{S_1, S_2, \cdots, S_m}$ are independent, we can write the joint density function of $S$ as 
	\begin{align*}
		p_S \parens{\bs} = \prod_{j=1}^m p_{S_j} \parens{s_j}
	\end{align*}
	as before. For each component, in order to represent the departure from the Gaussian distribution as far as possible, we let each component density as 
	\begin{align}\label{eq-tilted-gaussian}
		p_{S_j} \parens{s_j} = \varphi \parens{s_j} e^{g_j \parens{s_j}}, \qquad \text{ for all } j = 1, 2, \cdots, m, 
	\end{align}
	where $\varphi$ is the standard Gaussian density and $g_j$'s satisfy the normalization conditions required by a density function. The density in \eqref{eq-tilted-gaussian} is known as the \emph{tilted} Gaussian density function. 
	
	\item \textbf{Problem Formulation:} Let $X = \bA S$, where $A \in \Real^{m \times m}$ is assumed to be an \emph{orthogonal} matrix so that $\bA^\top = \bA^{-1}$. Then, with the data $\bx_1, \bx_2, \cdots, \bx_n \in \Real^m$, the log-likelihood function is 
	\begin{align*}
		L \parens{\bA} = \sum_{i=1}^n \sum_{j=1}^m \parens[\big]{\log \varphi_j \parens{\ba_j^\top \bx_i} + g_j \parens{\ba_j^\top \bx_i}}, 
	\end{align*}
	where $\ba_j$ is the $j$-th column of $\bA$. 
	We maximize $L$ above under the constraints that $\bA$ is orthogonal and 
	\begin{align*}
		\int \varphi \parens{s} e^{g_j \parens{s}} \diff s = 1, \qquad \text{ for all } j = 1, 2, \cdots, m. 
	\end{align*}
	Combining these constraints, we maximize the following objective function 
	\begin{align}\label{eq-prodenica}
		\sum_{j=1}^m \bracks[\Bigg]{\frac{1}{n} \sum_{i=1}^n \parens[\big]{\log \varphi_j \parens{\ba_j^\top \bx_i} + g_j \parens{\ba_j^\top \bx_i}} - \int \varphi \parens{s} e^{g_j \parens{s}} \diff s - \lambda_j \int \parens{g_j''' \parens{s}}^2 \diff s}, 
	\end{align}
	where $\lambda_j > 0$ is the penalty parameter and. In \eqref{eq-prodenica}, for each $j$, two penalty terms have subtracted: 
	\begin{enumerate}
		\item the first penalty enforces the density constraint $\int \varphi \parens{s} e^{g_j \parens{s}} \diff s = 1$, and 
		\item the second is a roughness penalty, which guarantees that the maximizer $\hat{g}_j$ is a \underline{quartic spline} with knots at the observed values of $s_{i,j} = \ba_j^\top \bx_i$. 
	\end{enumerate}
	
	\textit{Remark 1.} Note that, as $\lambda_j \to \infty$ for all $j = 1, 2, \cdots, m$, the resulting density function is approaching the standard Gaussian density. 
	
	\textit{Remark 2.} It can be shown that each solution densities $\hat{p}_{S_j} = \varphi e^{\hat{g}_j}$ has mean zero and variance one. 
	
	\item \textbf{Algorithm:} We fit the functions $g_j$ and directions $\ba_j$ by optimizing \eqref{eq-prodenica} in an alternating fashion, as described in the following algorithm. 
	
	\begin{minipage}{\linewidth}
		\begin{algorithm}[H]
		\caption{ProDenICA Algorithm}\label{algo-prodenica}
		\begin{algorithmic}[1]
		\STATE Initialize $\bA$ (random Gaussian matrix followed by orthogonalization); 
		\STATE Alternate until convergence of $\bA$: 
		\begin{enumerate}
			\item Given $\bA$, optimize \eqref{eq-prodenica} with respect to $g_j$ (separately for each $j$); 
			\item Given $g_j$, for each $j = 1, 2, \cdots, p$, perform one step of a fixed point algorithm towards finding the optimal $\bA$. 
		\end{enumerate}
		\end{algorithmic}
		\end{algorithm}
	\end{minipage}
	
	\item \textbf{Details of Step 2(a) in Algorithm \ref{algo-prodenica}:} In Step 2(a), with the matrix $\bA$ being fixed, we maximize with respect to $g_j$'s, which corresponds to $m$ semi-parametric density estimation problems. 
	
	Since $m$ components in \eqref{eq-prodenica} are separable, we can just consider a single $j$-th component and maximize 
	\begin{align}\label{eq-prodenica-compj}
		\frac{1}{n} \sum_{i=1}^n \parens[\big]{\log \varphi \parens{s_i} + g \parens{s_i}} - \int \varphi \parens{s} e^{g \parens{s}} \diff s - \lambda \int \parens{g''' \parens{s}}^2 \diff s. 
	\end{align}
	Even though the second integral leads to a smoothing spline, the first integral is problematic and requires an approximation. 
	
	We construct a fine grid of $T$ values $s^*_t$ in increments $\Delta$ covering the observed values $s_i$'s, and count the number of $s_i$ in the resulting bins 
	\begin{align*}
		y_t^* = \frac{\abs[\big]{\sets{s_i \,\vert\, s_i \in \parens{s^*_t - \Delta / 2, s^*_t + \Delta / 2}}}}{n}. 
	\end{align*}
	Then, we can approximate \eqref{eq-prodenica-compj} as 
	\begin{align*}
		\sum_{t=1}^T \bracks[\Big]{y_t^* \parens[\big]{\log \varphi \parens{s_t^*} + g \parens{s_t^*}} - \Delta \varphi \parens{s_t^*} e^{g \parens{s_t^*}} } - \lambda \int \parens{g''' \parens{s}}^2 \diff s. 
	\end{align*}
	
	\item \textbf{Details of Step 2(b) in Algorithm \ref{algo-prodenica}:} In Step 2(b), with $g_j$'s being fixed, we maximize with respect to $\bA$. By algebra and the assumption that $\bA$ is orthogonal, it is easy to show that the terms involving $\varphi$ do \emph{not} depend on $\bA$. We only need to maximize 
	\begin{align*}
		\frac{1}{n} \sum_{j=1}^m \sum_{i=1}^n g_j \parens{\ba_j^\top \bx_i}, \qquad \text{ with respect to } \ba_1, \ba_2, \cdots, \ba_m. 
	\end{align*}
	Then, for each $j$, we update $\ba_j$ using the Newton-Raphson algorithm given by 
	\begin{align*}
		\ba_j \longleftarrow \E_{\widehat{F}_n} \bracks{X g_j' \parens{\ba_{j}^\top X}} - \ba_{j} \E_{\widehat{F}_n} \bracks{ g_j'' \parens{\ba_{j}^\top X}}. 
	\end{align*}
	Since $g_j$ is a fitted quartic (or cubic) spline, the first and second derivatives are readily available.
	
	In order to make $\bA$ satisfy the orthogonality assumption, we orthogonalize $\bA$ using the symmetric square-root transformation 
	\begin{align*}
		\bA \longleftarrow \parens{\bA \bA^\top}^{-\frac{1}{2}} \bA. 
	\end{align*}
	If, in particular, $\bA = \bU \bD \bV^\top$ is the SVD of $\bA$, we have 
	\begin{align*}
		\bA \longleftarrow \bU \bV^\top. 
	\end{align*}
	
	
\end{enumerate}

\printbibliography

\end{document}
