\documentclass[12pt]{article}

\RequirePackage{amsmath}
\RequirePackage{amsthm}
\RequirePackage{amssymb}
\RequirePackage[mathscr]{eucal}
\RequirePackage{mathtools}
\RequirePackage{etoolbox}

\usepackage[red]{zhoucx-notation}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\renewcommand{\qedsymbol}{\hfill\rule{2mm}{2mm}}

\pagestyle{fancy}
\fancyhf{}
\setlength{\headheight}{15pt}
\rhead{\textsf{Chapter 21, Ensemble Learning}}
\lhead{\textsf{Chenxi Zhou}}
\renewcommand{\headrulewidth}{1pt}
\cfoot{\thepage}

\newcommand{\titlebox}[4]{
\begin{tcolorbox}[colback = blue!5!white!95, colframe = blue!70!black
% colback = yellow!30!white, colframe = yellow!70!black 
]
  \noindent \textbf{ #1 } \hfill \textit{#2} 
  \begin{center}
  	 \LARGE{\textbf{#3}}
  \end{center}
\textbf{Chapter:} \textit{#4} \hfill \textbf{Prepared by:} \textit{Chenxi Zhou}
\end{tcolorbox}
}

\begin{document}

\titlebox{Notes on Statistical and Machine Learning}{}{Ensemble Learning}{21}
\thispagestyle{plain}

\vspace{10pt}

This note is prepared based on \textit{Chapter 16, Ensemble Learning} in \textcite{Friedman2001-np}. 

\section*{I. Introduction}

\begin{enumerate}[label=\textbf{\arabic*.}]

	\item \textbf{Examples of Ensemble Learning:} 
	\begin{enumerate}
		\item Bagging; 
		\item Random forests; 
		\item Boosting; 
		\item Stacking; 
		\item $\cdots$
	\end{enumerate}
	
	\item \textbf{Two Tasks in Ensemble Learning:} Ensemble learning can be broken down into two tasks: 
	\begin{enumerate}
		\item developing a population of base learners from the training data, and 
		\item combining them to form the composite predictor. 
	\end{enumerate}

\end{enumerate}


\section*{II. Boosting and Regularized Paths}

\begin{enumerate}[label=\textbf{\arabic*.}]
	
	\item \textbf{Setup:} Consider a regression problem using the squared error loss function. 

	\item \textbf{Basis Functions:} Consider the dictionary of all possible $J$-terminal node regression trees 
	\begin{align}\label{eq-dict}
		\calT := \sets[\big]{T_1, T_2, \cdots, T_K} 
	\end{align}
	that could be realized on the training data as basis functions in $\Real^p$, where $K$ is a large number and denotes the number of trees. 
	
	\item \textbf{Model Specification:} The linear model is 
	\begin{align}\label{eq-model}
		f \parens{\bx} = \sum_{k=1}^K \alpha_k T_k \parens{\bx}, \qquad \text{ for all } \bx \in \Real^p. 
	\end{align}
	
	\item \textbf{Objective Function:} To avoid the overfitting issue, we impose regularization by adding a penalty term and solve the following minimization problem 
	\begin{align}\label{eq-obj-func}
		\minimize_{\balpha} \ \braces[\Bigg]{\sum_{i=1}^n \parens[\bigg]{y_i - \sum_{k=1}^K \alpha_k T_k \parens{\bx}}^2 + \lambda \cdot P \parens{\balpha}}, 
	\end{align}
	where $\balpha := \parens{\alpha_1, \alpha_2, \cdots, \alpha_K}^\top$ and $P: \Real^K \to [0, \infty)$ is a function of the coefficients that penalizes larger values. 
	
	\item \textbf{Examples of $P$:} Examples of $P$ include 
	\begin{enumerate}
		\item ridge 
		\begin{align*}
			P_{\mathrm{ridge}} \parens{\balpha} := \sum_{k=1}^K \alpha_k^2; 
		\end{align*} 
		\item lasso 
		\begin{align*}
			P_{\mathrm{lasso}} \parens{\balpha} := \sum_{k=1}^K \abs{\alpha_k}. 
		\end{align*}
	\end{enumerate}
	
	\textit{Remark.} As we increase the value of $\lambda$, non-zero coefficients are shrunk by the lasso penalty function toward 0. 
	
	If we use a sufficiently large value for $\lambda$, many of the $\widehat{\alpha}_k \parens{\lambda}$ will be equal to 0, where $\widehat{\alpha}_k \parens{\lambda}$ is the $k$-th element of the minimizer of \eqref{eq-obj-func}. That is, only a small fraction of all possible trees enter the model \eqref{eq-model}. In particular, we have 
	\begin{align*}
		\abs{\widehat{\alpha}_k \parens{\lambda}} < \abs{\widehat{\alpha}_k \parens{0}}, \qquad \text{ for all } k = 1, 2, \cdots, K \text{ and all } \lambda > 0. 
	\end{align*}
	
	\item \textbf{Algorithm:} Due to the very large number of basis functions in \eqref{eq-dict}, directly solving \eqref{eq-obj-func} with the lasso penalty $P_{\mathrm{lasso}}$ is \emph{not} possible. Instead, we use a forward stagewise strategy to closely \emph{approximate} the effect of the lasso, which is very similar to boosting. 
	
	\begin{minipage}{\linewidth}
	\begin{algorithm}[H]
		\caption{Forward Stagewise Linear Regression}\label{algo-fslr}
		\begin{algorithmic}[1]
			\REQUIRE Initialize $\check{\alpha}_k = 0$, for all $k = 1, \cdots, K$; 
			\REQUIRE Set $\varepsilon > 0$ to some small constant; 
			\REQUIRE A large integer $M$. 
			
			\FOR{$m = 1$ to $M$}
			\STATE Compute 
			\begin{align*}
				\parens{\beta^*, k^*} := \argmin_{\beta, k} \sum_{i=1}^n \parens[\bigg]{y_i - \sum_{\ell=1}^K \check{\alpha}_{\ell} T_{\ell} \parens{\bx_i} - \beta T_k \parens{\bx_i}}^2; 
			\end{align*}
			\STATE Update 
			\begin{align*}
				\check{\alpha}_{k^*} \quad \leftarrow \quad \check{\alpha}_{k^*} + \varepsilon \cdot \sign \parens{\beta^*}. 
			\end{align*}
			\ENDFOR
			\RETURN $f_M \parens{\bx} = \sum_{k=1}^K \check{\alpha}_{k} T_k \parens{\bx}$. 
		\end{algorithmic}
	\end{algorithm}
	\end{minipage}
	
	\vspace{10pt}
	
	\textit{Remark 1.} Although phrased in terms of tree basis functions $T_k$, Algorithm \ref{algo-fslr} can be used with \emph{any} set of basis functions. 
	
	\textit{Remark 2.} In Algorithm \ref{algo-fslr}, initially all coefficients are zero; this corresponds to $\lambda = \infty$ in \eqref{eq-obj-func}. At each successive step, the tree $T_{k^*}$ is selected that best fits the current residuals. Its corresponding coefficient $\check{\alpha}_{k^*}$ is then incremented or decremented by an infinitesimal amount, while all other coefficients $\check{\alpha}_{k}$, where $k \neq k^*$, are left unchanged. 

\end{enumerate}


\section*{III. Learning Ensembles --- Importance Sampled Learning Ensemble}

\begin{enumerate}[label=\textbf{\arabic*.}]
	
	\item \textbf{Setup:} We consider functions of the form 
	\begin{align*}
		f \parens{\bx} = \alpha_0 + \sum_{T_k \in \calT} \alpha_k T_k \parens{\bx}, 
	\end{align*}
	where $\calT$ is a dictionary of basis functions, typically trees. 
	
	\textit{Remark.} For gradient boosting machine and random forests, the cardinality of $\calT$ is very large, and it is quite typical for the final model to involve many thousands of trees. We would like to shrink some coefficients $\alpha_k$ to be 0. 
	
	\item \textbf{General Procedure of Importance Sampled Learning Ensemble:} The general procedure of \emph{importance sampled learning ensemble} involves the following two steps: 
	\begin{enumerate}
		\item Induce a finite dictionary 
		\begin{align*}
			\calT_L := \sets{T_1 \parens{\bx}, T_2 \parens{\bx}, \cdots, T_M \parens{\bx}}
		\end{align*}
		of basis functions from the training data; 
		\item Build a family of functions $f_{\lambda}$ by fitting a lasso path in $\calT_L$: 
		\begin{align*}
			\balpha \parens{\lambda} := \argmin_{\balpha} \braces[\Bigg]{L \parens[\bigg]{Y_i, \alpha_0 + \sum_{m=1}^M \alpha_m T_m \parens{\bx_i}} + \lambda \sum_{m=1}^M \abs{\alpha_m}}, 
		\end{align*}
		where $\balpha := \parens{\alpha_0, \alpha_1, \cdots, \alpha_M}^\top$. 
	\end{enumerate}
	
	\textit{Remark.} The procedure outlined above can be viewed as a way of post-processing gradient boosting machine or random forests, where $\calT_L$ is the collection of trees produced by the corresponding algorithm. 
	
	By fitting the lasso path to these trees, we would typically use a \emph{much reduced} set, which would save in computations and storage for future predictions. 
	
	\item \textbf{How to Learn $\calT_L$:} 
	\begin{enumerate}
		\item \textit{General Philosophy:} A good choice of basis functions in $\calT_L$ should satisfy the following criteria: 
		\begin{enumerate}
			\item basis functions in $\calT_L$ should cover the space well in places where they are needed, and 
			\item basis functions in $\calT_L$ should be sufficiently different from each other for the post-processor to be effective. 
		\end{enumerate}
		
		\item \textit{Linking to Quadrature:} We write the unknown regression function $f$ as 
		\begin{align}\label{eq-num-qua}
			f \parens{\bx} = \alpha_0 + \int \beta \parens{\gamma} b \parens{\bx; \gamma} \diff \gamma, 
		\end{align}
		where $\gamma \in \Gamma$ indexes the basis functions $b \parens{\,\cdot\,; \gamma}$. Then, numerical quadrature approximates $f$ in \eqref{eq-num-qua} by 
		\begin{align*}
			f \parens{\bx} \approx & \, \alpha_0 + \sum_{m=1}^M w_m \beta \parens{\gamma_m} b \parens{\bx; \gamma_m} \\ 
			= & \, c_0 + \sum_{m=1}^M c_m b \parens{\bx; \gamma_m}, 
		\end{align*}
		where $\sets{\gamma_1, \gamma_2, \cdots, \gamma_m}$ is a set of evaluation points, $w_m$ is the weight for the $m$-th evaluation point $\gamma_m$, $c_0 = \alpha_0$ and 
		\begin{align*}
			c_m := w_m \beta \parens{\gamma_m}, \qquad \text{ for all } m = 1, 2, \cdots, M. 
		\end{align*}
		We consider $c_m$, instead of $w_m$ and $\beta \parens{\gamma_m}$ separately, because $w_m$ and $\beta \parens{\gamma_m}$ are \emph{not} separately identifiable. 
		
		With this setup, for a given set of evaluation points $\gamma_1, \gamma_2, \cdots, \gamma_M$, we can estimate $c_0, c_1, \cdots, c_M$ by minimizing 
		\begin{align*}
			\sum_{i=1}^n L \parens[\Bigg]{Y_i, c_0 + \sum_{m=1}^M c_m b \parens{\bx_i; \gamma_m}}. 
		\end{align*}
		
		\textit{Remark.} The remaining question becomes how to choose the evaluation points $\gamma_1, \gamma_2, \cdots, \gamma_M$. 
		
		\item \textit{Example:} If the basis functions are trees, then $\gamma$ indexes the splitting variables, the split-points and the values in the terminal nodes. Then, numerical quadrature amounts to finding a set of $M$ evaluation points $\gamma_m \in \Gamma$ and corresponding weights $c_m$ so that $f_M \parens{\bx} = c_0 + \sum_{m=1}^M c_m b \parens{\bx; \gamma_m}$ approximates $f$ well over the domain of $\bx$. 
		
		\item \textit{Measurement of Lack of Relevance:} Given only a single potential evaluation point $\gamma \in \Gamma$, without the knowledge of other points that will be used with it in the integration rule, we measure its \emph{lack of relevance} by 
		\begin{align*}
			Q \parens{\gamma} := \min_{c_0, c_1} \ \braces[\bigg]{\sum_{i=1}^n L \parens[\big]{Y_i, c_0 + c_1 b \parens{\bx_i; \gamma}}}, 
		\end{align*}
		which is the prediction risk of using $\gamma \in \Gamma$ alone. 
		
		The optimal single point rule is obtained by 
		\begin{align*}
			\gamma^* := \argmin_{\gamma \in \Gamma} Q \parens{\gamma}. 
		\end{align*}
		
		\item \textit{ISLE Ensemble Generation:} We use sub-sampling to introduce the randomness and to generate an ensemble of basis functions. The reason why we introduce randomness is to let the resulting basis functions be as different as possible. 
		
		The algorithm is outlined in Algorithm \ref{algo-lsle}. 
		
		\begin{minipage}{\linewidth}
			\begin{algorithm}[H]
			\caption{Importance Sampled Learning Ensemble}\label{algo-lsle}
			\begin{algorithmic}[1]
				\STATE Set $f_0 = \argmin_c \sum_{i=1}^n L \parens{Y_i, c}$; 
				\FOR{$m=1$ to $M$}
				\STATE Compute 
				\begin{align*}
					\gamma_m := \argmin_{\gamma} \braces[\Bigg]{\sum_{i \in S_m \parens{\eta}} L \parens{Y_i, f_{m-1} \parens{\bx_i} + b \parens{\bx_i; \gamma}}}; 
				\end{align*}
				\STATE Set 
				\begin{align*}
					f_m \parens{\bx} = f_{m-1} \parens{\bx} + \nu b \parens{\bx; \gamma_m}. 
				\end{align*}
				\ENDFOR
				
				\RETURN $\calT_{\mathrm{ISLE}} := \sets{b \parens{\,\cdot\,; \gamma_1}, b \parens{\,\cdot\,; \gamma_2}, \cdots, b \parens{\,\cdot\,; \gamma_M}}$. 
			\end{algorithmic}
		\end{algorithm}
		\end{minipage}
		
		\vspace{10pt}
		
		In Algorithm \ref{algo-lsle}, $S_m \parens{\eta}$ refers to a subsample of $n \cdot \eta$, where $\eta \in (0, 1]$, of the training observations, typically \emph{without} replacement. Suggested values of $\eta$ is $\eta \le \frac{1}{2}$, and for large $n$ pick $\eta \sim n^{-\frac{1}{2}}$. 
		
		\textit{Remark 1.} Reducing $\eta$ increases the randomness. 
		
		\textit{Remark 2.} The parameter $\nu \in \bracks{0, 1}$ introduces memory into the randomization process; the larger $\nu$, the more the procedure avoids the newly learned basis function $b \parens{\,\cdot\,; \gamma}$ similar to those found before. 
		
	\end{enumerate}
	
\end{enumerate}


\printbibliography

\end{document}
